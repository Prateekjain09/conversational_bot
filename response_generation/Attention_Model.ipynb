{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /home/prateek/snap/jupyter/common/lib/python3.7/site-packages (4.1.1)\r\n",
      "Requirement already satisfied: text-unidecode==1.3 in /home/prateek/snap/jupyter/common/lib/python3.7/site-packages (from faker) (1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /snap/jupyter/6/lib/python3.7/site-packages (from faker) (2.8.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /snap/jupyter/6/lib/python3.7/site-packages (from python-dateutil>=2.4->faker) (1.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/prateek/snap/jupyter/common/lib/python3.7/site-packages (4.46.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: babel in /home/prateek/snap/jupyter/common/lib/python3.7/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2015.7 in /home/prateek/snap/jupyter/common/lib/python3.7/site-packages (from babel) (2020.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install babel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prateek/conversational_bot'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "i = 0\n",
    "with open(\"subtitles\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.lower()\n",
    "        dataset.append(values)\n",
    "        i = i + 1\n",
    "        if i == 1000000:\n",
    "          break\n",
    "print('Found %s word vectors.' % len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from one of the tensorflow pa\n",
    "vocab_size = 20000                    \n",
    "embedding_dim = 200\n",
    "max_length = 20               \n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "MAX_SEQUENCE_LENGTH=20\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(dataset)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(dataset)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#taken from https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "embeddings_index = {}\n",
    "with open(\"glove.6B.200d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    \n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1,200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            200,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they made the men dig their own graves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[999999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {}\n",
    "for word , index in word_index.items():\n",
    "    index_word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 48  30 268   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[ 30  87   5 169   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "(10000, 20)\n",
      "(10000, 20)\n"
     ]
    }
   ],
   "source": [
    "input_seq = []\n",
    "input_labs = []\n",
    "j = 0\n",
    "for i in padded:\n",
    "  if j%2 == 0:\n",
    "    input_seq.append(i)\n",
    "  if j%2 == 1:\n",
    "    input_labs.append(i)\n",
    "  j+=1\n",
    "  if j == 20000:\n",
    "    break\n",
    "\n",
    "input_seq = np.array(input_seq)\n",
    "input_labs = np.array(input_labs)\n",
    "\n",
    "print(input_seq[8])\n",
    "print(input_labs[3])\n",
    "print(input_seq.shape)\n",
    "print(input_labs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2726   100 12873  1352     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(input_labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(20)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attention) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas,a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 128 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 128 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "# Please note, this is the post attention LSTM cell.  \n",
    "# For the purposes of passing the automatic grader\n",
    "# please do not modify this global variable.  This will be corrected once the automatic grader is also updated.\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
    "#output_layer = Dense(len(input_labs), activation=softmax)\n",
    "output_layer = Dense(20000, activation=softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
    "    # for the decoder LSTM with shape (n_s,)\n",
    "    X = Input(shape=(Tx,))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    e = embedding_layer(X)\n",
    "    \n",
    "    a = Bidirectional(LSTM(units = n_a ,return_sequences = True))(e)\n",
    "    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
    "    #a = Bidirectional(LSTM(units=n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s) \n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "        model = Model(inputs=[X,s0,c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(20, 20, n_a, n_s, 20000, 20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 200)      9013800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 20, 256)      336896      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 20, 128)      0           s0[0][0]                         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[18][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 20, 384)      0           bidirectional[0][0]              \n",
      "                                                                 repeat_vector[0][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[1][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[2][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[3][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[4][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[5][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[6][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[7][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[8][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[9][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[10][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[11][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[12][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[13][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[14][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[15][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[16][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[17][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[18][0]             \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[19][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20, 10)       3850        concatenate[0][0]                \n",
      "                                                                 concatenate[1][0]                \n",
      "                                                                 concatenate[2][0]                \n",
      "                                                                 concatenate[3][0]                \n",
      "                                                                 concatenate[4][0]                \n",
      "                                                                 concatenate[5][0]                \n",
      "                                                                 concatenate[6][0]                \n",
      "                                                                 concatenate[7][0]                \n",
      "                                                                 concatenate[8][0]                \n",
      "                                                                 concatenate[9][0]                \n",
      "                                                                 concatenate[10][0]               \n",
      "                                                                 concatenate[11][0]               \n",
      "                                                                 concatenate[12][0]               \n",
      "                                                                 concatenate[13][0]               \n",
      "                                                                 concatenate[14][0]               \n",
      "                                                                 concatenate[15][0]               \n",
      "                                                                 concatenate[16][0]               \n",
      "                                                                 concatenate[17][0]               \n",
      "                                                                 concatenate[18][0]               \n",
      "                                                                 concatenate[19][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20, 1)        11          dense[0][0]                      \n",
      "                                                                 dense[1][0]                      \n",
      "                                                                 dense[2][0]                      \n",
      "                                                                 dense[3][0]                      \n",
      "                                                                 dense[4][0]                      \n",
      "                                                                 dense[5][0]                      \n",
      "                                                                 dense[6][0]                      \n",
      "                                                                 dense[7][0]                      \n",
      "                                                                 dense[8][0]                      \n",
      "                                                                 dense[9][0]                      \n",
      "                                                                 dense[10][0]                     \n",
      "                                                                 dense[11][0]                     \n",
      "                                                                 dense[12][0]                     \n",
      "                                                                 dense[13][0]                     \n",
      "                                                                 dense[14][0]                     \n",
      "                                                                 dense[15][0]                     \n",
      "                                                                 dense[16][0]                     \n",
      "                                                                 dense[17][0]                     \n",
      "                                                                 dense[18][0]                     \n",
      "                                                                 dense[19][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 20, 1)        0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "                                                                 dense_1[10][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "                                                                 dense_1[14][0]                   \n",
      "                                                                 dense_1[15][0]                   \n",
      "                                                                 dense_1[16][0]                   \n",
      "                                                                 dense_1[17][0]                   \n",
      "                                                                 dense_1[18][0]                   \n",
      "                                                                 dense_1[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 256)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[10][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[11][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[12][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[13][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[14][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[15][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[16][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[17][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[18][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[19][0]         \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 197120      dot[0][0]                        \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[1][0]                        \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "                                                                 dot[2][0]                        \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[1][2]                       \n",
      "                                                                 dot[3][0]                        \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[2][2]                       \n",
      "                                                                 dot[4][0]                        \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[3][2]                       \n",
      "                                                                 dot[5][0]                        \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[4][2]                       \n",
      "                                                                 dot[6][0]                        \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[5][2]                       \n",
      "                                                                 dot[7][0]                        \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[6][2]                       \n",
      "                                                                 dot[8][0]                        \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[7][2]                       \n",
      "                                                                 dot[9][0]                        \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[8][2]                       \n",
      "                                                                 dot[10][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "                                                                 lstm[9][2]                       \n",
      "                                                                 dot[11][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[10][2]                      \n",
      "                                                                 dot[12][0]                       \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[11][2]                      \n",
      "                                                                 dot[13][0]                       \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[12][2]                      \n",
      "                                                                 dot[14][0]                       \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[13][2]                      \n",
      "                                                                 dot[15][0]                       \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[14][2]                      \n",
      "                                                                 dot[16][0]                       \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[15][2]                      \n",
      "                                                                 dot[17][0]                       \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[16][2]                      \n",
      "                                                                 dot[18][0]                       \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[17][2]                      \n",
      "                                                                 dot[19][0]                       \n",
      "                                                                 lstm[18][0]                      \n",
      "                                                                 lstm[18][2]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20000)        2580000     lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[18][0]                      \n",
      "                                                                 lstm[19][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,131,677\n",
      "Trainable params: 3,117,877\n",
      "Non-trainable params: 9,013,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=10000\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(input_labs.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1 = 0.9,beta_2 = 0.999 , decay = 0.01)\n",
    "model.compile(optimizer = opt , loss = \"sparse_categorical_crossentropy\" ,metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 54s 542ms/step - loss: 40.6884 - dense_2_loss: 6.2430 - dense_2_1_loss: 6.1259 - dense_2_2_loss: 5.2208 - dense_2_3_loss: 4.4638 - dense_2_4_loss: 3.7071 - dense_2_5_loss: 2.9335 - dense_2_6_loss: 2.3084 - dense_2_7_loss: 1.8941 - dense_2_8_loss: 1.4909 - dense_2_9_loss: 1.1991 - dense_2_10_loss: 0.9610 - dense_2_11_loss: 0.7719 - dense_2_12_loss: 0.6324 - dense_2_13_loss: 0.5375 - dense_2_14_loss: 0.4814 - dense_2_15_loss: 0.4177 - dense_2_16_loss: 0.3697 - dense_2_17_loss: 0.3361 - dense_2_18_loss: 0.3083 - dense_2_19_loss: 0.2860 - dense_2_accuracy: 0.0039 - dense_2_1_accuracy: 0.1492 - dense_2_2_accuracy: 0.3238 - dense_2_3_accuracy: 0.4456 - dense_2_4_accuracy: 0.5556 - dense_2_5_accuracy: 0.6571 - dense_2_6_accuracy: 0.7363 - dense_2_7_accuracy: 0.7949 - dense_2_8_accuracy: 0.8402 - dense_2_9_accuracy: 0.8795 - dense_2_10_accuracy: 0.9089 - dense_2_11_accuracy: 0.9299 - dense_2_12_accuracy: 0.9461 - dense_2_13_accuracy: 0.9571 - dense_2_14_accuracy: 0.9644 - dense_2_15_accuracy: 0.9713 - dense_2_16_accuracy: 0.9770 - dense_2_17_accuracy: 0.9810 - dense_2_18_accuracy: 0.9846 - dense_2_19_accuracy: 0.9867\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 59s 595ms/step - loss: 38.8871 - dense_2_loss: 6.0285 - dense_2_1_loss: 5.8311 - dense_2_2_loss: 5.0246 - dense_2_3_loss: 4.3224 - dense_2_4_loss: 3.6187 - dense_2_5_loss: 2.8815 - dense_2_6_loss: 2.2734 - dense_2_7_loss: 1.8612 - dense_2_8_loss: 1.4571 - dense_2_9_loss: 1.1590 - dense_2_10_loss: 0.9099 - dense_2_11_loss: 0.7164 - dense_2_12_loss: 0.5708 - dense_2_13_loss: 0.4713 - dense_2_14_loss: 0.4117 - dense_2_15_loss: 0.3474 - dense_2_16_loss: 0.2974 - dense_2_17_loss: 0.2614 - dense_2_18_loss: 0.2331 - dense_2_19_loss: 0.2101 - dense_2_accuracy: 0.0039 - dense_2_1_accuracy: 0.1492 - dense_2_2_accuracy: 0.3238 - dense_2_3_accuracy: 0.4456 - dense_2_4_accuracy: 0.5556 - dense_2_5_accuracy: 0.6571 - dense_2_6_accuracy: 0.7363 - dense_2_7_accuracy: 0.7949 - dense_2_8_accuracy: 0.8402 - dense_2_9_accuracy: 0.8795 - dense_2_10_accuracy: 0.9089 - dense_2_11_accuracy: 0.9299 - dense_2_12_accuracy: 0.9461 - dense_2_13_accuracy: 0.9571 - dense_2_14_accuracy: 0.9644 - dense_2_15_accuracy: 0.9713 - dense_2_16_accuracy: 0.9770 - dense_2_17_accuracy: 0.9810 - dense_2_18_accuracy: 0.9846 - dense_2_19_accuracy: 0.9867\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 57s 565ms/step - loss: 38.2136 - dense_2_loss: 5.9327 - dense_2_1_loss: 5.7376 - dense_2_2_loss: 4.9700 - dense_2_3_loss: 4.2871 - dense_2_4_loss: 3.5897 - dense_2_5_loss: 2.8596 - dense_2_6_loss: 2.2576 - dense_2_7_loss: 1.8448 - dense_2_8_loss: 1.4417 - dense_2_9_loss: 1.1418 - dense_2_10_loss: 0.8896 - dense_2_11_loss: 0.6940 - dense_2_12_loss: 0.5452 - dense_2_13_loss: 0.4457 - dense_2_14_loss: 0.3827 - dense_2_15_loss: 0.3182 - dense_2_16_loss: 0.2668 - dense_2_17_loss: 0.2298 - dense_2_18_loss: 0.2009 - dense_2_19_loss: 0.1780 - dense_2_accuracy: 0.0039 - dense_2_1_accuracy: 0.1492 - dense_2_2_accuracy: 0.3238 - dense_2_3_accuracy: 0.4456 - dense_2_4_accuracy: 0.5556 - dense_2_5_accuracy: 0.6571 - dense_2_6_accuracy: 0.7363 - dense_2_7_accuracy: 0.7949 - dense_2_8_accuracy: 0.8402 - dense_2_9_accuracy: 0.8795 - dense_2_10_accuracy: 0.9089 - dense_2_11_accuracy: 0.9299 - dense_2_12_accuracy: 0.9461 - dense_2_13_accuracy: 0.9571 - dense_2_14_accuracy: 0.9644 - dense_2_15_accuracy: 0.9713 - dense_2_16_accuracy: 0.9770 - dense_2_17_accuracy: 0.9810 - dense_2_18_accuracy: 0.9846 - dense_2_19_accuracy: 0.9867\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 57s 568ms/step - loss: 37.9232 - dense_2_loss: 5.8556 - dense_2_1_loss: 5.7015 - dense_2_2_loss: 4.9490 - dense_2_3_loss: 4.2715 - dense_2_4_loss: 3.5713 - dense_2_5_loss: 2.8496 - dense_2_6_loss: 2.2503 - dense_2_7_loss: 1.8373 - dense_2_8_loss: 1.4345 - dense_2_9_loss: 1.1360 - dense_2_10_loss: 0.8855 - dense_2_11_loss: 0.6889 - dense_2_12_loss: 0.5395 - dense_2_13_loss: 0.4365 - dense_2_14_loss: 0.3761 - dense_2_15_loss: 0.3088 - dense_2_16_loss: 0.2562 - dense_2_17_loss: 0.2190 - dense_2_18_loss: 0.1898 - dense_2_19_loss: 0.1663 - dense_2_accuracy: 0.0039 - dense_2_1_accuracy: 0.1492 - dense_2_2_accuracy: 0.3238 - dense_2_3_accuracy: 0.4456 - dense_2_4_accuracy: 0.5556 - dense_2_5_accuracy: 0.6571 - dense_2_6_accuracy: 0.7363 - dense_2_7_accuracy: 0.7949 - dense_2_8_accuracy: 0.8402 - dense_2_9_accuracy: 0.8795 - dense_2_10_accuracy: 0.9089 - dense_2_11_accuracy: 0.9299 - dense_2_12_accuracy: 0.9461 - dense_2_13_accuracy: 0.9571 - dense_2_14_accuracy: 0.9644 - dense_2_15_accuracy: 0.9713 - dense_2_16_accuracy: 0.9770 - dense_2_17_accuracy: 0.9810 - dense_2_18_accuracy: 0.9846 - dense_2_19_accuracy: 0.9867\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 56s 561ms/step - loss: 37.7045 - dense_2_loss: 5.8008 - dense_2_1_loss: 5.6821 - dense_2_2_loss: 4.9387 - dense_2_3_loss: 4.2590 - dense_2_4_loss: 3.5595 - dense_2_5_loss: 2.8334 - dense_2_6_loss: 2.2390 - dense_2_7_loss: 1.8261 - dense_2_8_loss: 1.4276 - dense_2_9_loss: 1.1292 - dense_2_10_loss: 0.8799 - dense_2_11_loss: 0.6816 - dense_2_12_loss: 0.5333 - dense_2_13_loss: 0.4310 - dense_2_14_loss: 0.3692 - dense_2_15_loss: 0.3035 - dense_2_16_loss: 0.2513 - dense_2_17_loss: 0.2134 - dense_2_18_loss: 0.1845 - dense_2_19_loss: 0.1613 - dense_2_accuracy: 0.0039 - dense_2_1_accuracy: 0.1492 - dense_2_2_accuracy: 0.3238 - dense_2_3_accuracy: 0.4456 - dense_2_4_accuracy: 0.5556 - dense_2_5_accuracy: 0.6571 - dense_2_6_accuracy: 0.7363 - dense_2_7_accuracy: 0.7949 - dense_2_8_accuracy: 0.8402 - dense_2_9_accuracy: 0.8795 - dense_2_10_accuracy: 0.9089 - dense_2_11_accuracy: 0.9299 - dense_2_12_accuracy: 0.9461 - dense_2_13_accuracy: 0.9571 - dense_2_14_accuracy: 0.9644 - dense_2_15_accuracy: 0.9713 - dense_2_16_accuracy: 0.9770 - dense_2_17_accuracy: 0.9810 - dense_2_18_accuracy: 0.9846 - dense_2_19_accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f181caedd30>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([input_seq, s0, c0], outputs, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 0.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        row=row.reshape(20000,)\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score - log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "h= [(\"hey there, how are you?\")]\n",
    "seq= tokenizer.texts_to_sequences(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[175, 52, 63, 31, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = pad_sequences(seq,maxlen=20, truncating=\"post\",padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([pad , np.zeros((1,128)) , np.zeros((1,128))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  5.9650143690487445],\n",
       " [[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  8.854289221373946],\n",
       " [[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  8.97263521536872]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = beam_search_decoder(pred,3)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
