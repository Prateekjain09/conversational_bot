{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa   \n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from scipy.io import wavfile #for audio processing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prateek/conversational_bot/speech_recognition'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prateek/conversational_bot/speech_recognition/sample_data'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/sample_data'\n",
    "os.chdir(os.getcwd()+path )\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['227', '198']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string : 111\n",
      "227/19-227-0021.flac \n",
      " 198/19-198-0001.flac\n"
     ]
    }
   ],
   "source": [
    "list_IDs = []\n",
    "\n",
    "for direc in os.listdir():\n",
    "        file = [ f for f in os.listdir(os.getcwd() + '/' + direc ) if f.endswith('.flac')]\n",
    "        for f in file:\n",
    "            list_IDs.append(direc + '/' + f)\n",
    "\n",
    "print(\"string :\",len(list_IDs))\n",
    "print((list_IDs[0]),\"\\n\",(list_IDs[110]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#%%\n",
    "# From Baidu ba-dls-deepspeech - https://github.com/baidu-research/ba-dls-deepspeech\n",
    "# Character map list\n",
    "\n",
    "char_map_str = \"\"\"\n",
    "<SPACE> 0\n",
    "a 1\n",
    "b 2\n",
    "c 3\n",
    "d 4\n",
    "e 5\n",
    "f 6\n",
    "g 7\n",
    "h 8\n",
    "i 9\n",
    "j 10\n",
    "k 11\n",
    "l 12\n",
    "m 13\n",
    "n 14\n",
    "o 15\n",
    "p 16\n",
    "q 17\n",
    "r 18\n",
    "s 19\n",
    "t 20\n",
    "u 21\n",
    "v 22\n",
    "w 23\n",
    "x 24\n",
    "y 25\n",
    "z 26\n",
    "' 27\n",
    "\"\"\"\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "\n",
    "for line in char_map_str.strip().split('\\n'):\n",
    "    ch, index = line.split()\n",
    "    char_map[ch] = int(index)\n",
    "    index_map[int(index)] = ch\n",
    "\n",
    "index_map[0] = ' '\n",
    "\n",
    "\n",
    "def get_param(Y):\n",
    "  labels = np.zeros((520,23))\n",
    "\n",
    "  for idx,label in enumerate(Y):\n",
    "    new = []\n",
    "    for word in label.split():\n",
    "      for c in word:\n",
    "        if c not in char_map:\n",
    "          continue\n",
    "        elif c == \"'\":\n",
    "          continue\n",
    "        else:\n",
    "          ch = char_map[c]\n",
    "          new.append(ch)\n",
    "#       if(len(new) < max_label):\n",
    "#         new.append(0)\n",
    "    while(len(new) < max_label):\n",
    "      new.append(27)\n",
    "    labels[idx,:] = np.array(new)\n",
    "\n",
    "  input_length = np.array([max_label for _ in Y])\n",
    "  label_length = np.array([max_label for _ in Y])\n",
    "\n",
    "\n",
    "  return labels , input_length , label_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<SPACE>': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " \"'\": 27}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataGenerator(keras.utils.Sequence):\n",
    "  #batch_size, character index, max string length, \n",
    "  def __init__(self, source_dir, char_idx, batch_size=100, target_shape=(101, 198), max_string_len=10, augment=False, shuffle=True):\n",
    "    self.data_dir = source_dir\n",
    "    self.char_idx = char_idx\n",
    "    self.batch_size = batch_size\n",
    "    self.max_string_len = max_string_len\n",
    "    self.augment = augment\n",
    "    self.dims = target_shape\n",
    "    self.shuffle = shuffle\n",
    "\n",
    "    #Count number of training files\n",
    "    self.total_examples = 0\n",
    "    self.list_IDs = []\n",
    "\n",
    "    self.dir_list = os.listdir(self.data_dir)\n",
    "    for i, dir in enumerate(self.dir_list):\n",
    "        example_list = os.listdir(self.data_dir +'/'+ dir)\n",
    "        example_IDs = [[i, e] for e in example_list]\n",
    "        num_examples = len(example_list)\n",
    "      \n",
    "        self.total_examples += num_examples\n",
    "        self.list_IDs += example_IDs\n",
    "\n",
    "    print (\"found \" + str(self.total_examples) + \" examples\")\n",
    "\n",
    "    #call on_epcoh_end to perpare initial data indexes\n",
    "    self.on_epoch_end()\n",
    "\n",
    "  def __len__(self):\n",
    "    #number of training steps per epoch\n",
    "    return int(np.floor(self.total_examples/self.batch_size))\n",
    "    #return 100\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Generate indexes of the batch\n",
    "    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "    # Find list of IDs\n",
    "    list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "    # Generate data\n",
    "    X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "    #for CTC you to give 4 arguments, I have only given inputs and labels\n",
    "    #input shape and label shape should also be output by this function\n",
    "#     input_length = np.array([self.total_timedistributed_output for _ in range(self.batch_size)])\n",
    "#     label_length = np.array([self.max_label for _ in range(self.batch_size)])\n",
    "        \n",
    "    input_shape = np.zeros([self.batch_size, 1])\n",
    "    label_length = np.zeros([self.batch_size, 1])\n",
    "    input_shape[:] = 101\n",
    "    label_length[:] = self.max_string_len\n",
    "    inputs = {\"the_inputs\": X, \"the_labels\": y, \"input_length\":input_shape, \"label_length\": label_length}\n",
    "    outputs = {\"ctc\": y}\n",
    "    return (inputs, outputs)\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.list_IDs))\n",
    "    if self.shuffle == True:\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "  def __data_generation(self, list_IDs_temp):\n",
    "    # Initialization\n",
    "    X = np.empty((self.batch_size, *self.dims))\n",
    "    y = np.empty((self.batch_size, self.max_string_len), dtype=int)\n",
    "\n",
    "    # Generate data\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        wav_file = self.data_dir + self.dir_list[ID[0]] + \"/\" + ID[1]\n",
    "        spec_gram = self.__graph_spectrogram(wav_file)\n",
    "      \n",
    "      #I have fixed the dimensions, you will need to change this\n",
    "    if (spec_gram.shape == (101, 198)):\n",
    "        X[i,] = spec_gram\n",
    "    else:\n",
    "        X[i,] = np.zeros((101, 198))\n",
    "\n",
    "    y[i,] = self.__str_to_label(self.dir_list[ID[0]])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "  def __graph_spectrogram(self, flac_file):\n",
    "    rate, data = sf.read(flac_file)\n",
    "    nfft = 200 # Length of each window segment\n",
    "    fs = 8000  #Sampling frequencies\n",
    "    noverlap = 120 # Overlap between windows\n",
    "    nchannels = data.ndim\n",
    "    if nchannels == 1:\n",
    "        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n",
    "    elif nchannels == 2:\n",
    "        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n",
    "    return pxx\n",
    "\n",
    "  #get labels from input word\n",
    "\n",
    "  def __str_to_label(self, word):\n",
    "    #array of length max string length, 0 padded (actually it should be blank token :p)\n",
    "    label = np.zeros((self.max_string_len), dtype=int)\n",
    "    for i in range(len(word)):\n",
    "      label[i] = int(self.char_idx[word[i]])\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.merge import Add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args    \n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length )    \n",
    "    \n",
    "        \n",
    "#%%\n",
    "class model():\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "        sr_ctc = CTC(enter input_size and output_size)\n",
    "        sr_ctc.build()\n",
    "        sr_ctc.m.compile()\n",
    "        sr_ctc.tm.compile()\n",
    "    \"\"\"       \n",
    "    def __init__(self,\n",
    "                 input_size=None, \n",
    "                 output_size=None,\n",
    "                 initializer='glorot_uniform'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.initializer = initializer\n",
    "        self.m = None\n",
    "        self.tm = None\n",
    "                   \n",
    "    def build(self, \n",
    "              conv_filters = 200,\n",
    "              conv2d_filters = 13,\n",
    "              conv_size = 5,\n",
    "              conv2d_strides = 1,\n",
    "              conv_strides = 4,\n",
    "              act = 'relu',\n",
    "              rnn_layers = 2,\n",
    "              LSTM_units = 128,\n",
    "              drop_out = 0.8):\n",
    "           \n",
    "        i = Input(shape = self.input_size, name = 'input')\n",
    "        x = Conv2D(conv2d_filters,\n",
    "                   conv_size,\n",
    "                   strides = (conv2d_strides, conv2d_strides),\n",
    "                   padding = \"same\",\n",
    "                   name = \"conv2d1\")(i)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(act)(x)\n",
    "        x = Conv2D(conv2d_filters,\n",
    "                   conv_size,\n",
    "                   strides = (conv2d_strides , conv2d_strides),\n",
    "                   padding = \"same\",\n",
    "                   name = \"conv2d2\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(act)(x)\n",
    "        x = Reshape([606,-1])(x)\n",
    "        x = Conv1D(conv_filters, \n",
    "                   conv_size, \n",
    "                   strides = conv_strides,\n",
    "                   padding = \"same\", \n",
    "                   name = 'conv1d1')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(act)(x)\n",
    "        x = Conv1D(conv_filters, \n",
    "                   conv_size, \n",
    "                   strides = conv_strides,\n",
    "                   padding = \"same\", \n",
    "                   name = 'conv1d2')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(act)(x)\n",
    "        x = Reshape([200,38])(x)\n",
    "        x = Dense(23 , activation=\"softmax\", name =\"dense\")(x)\n",
    "        x = Reshape([23,-1])(x)\n",
    "        for _ in range(rnn_layers):          \n",
    "            x = Bidirectional(LSTM(LSTM_units, \n",
    "                                   return_sequences = True))(x)\n",
    "            x = Dropout(drop_out)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "        y_pred = TimeDistributed(Dense(self.output_size, \n",
    "                                       activation = 'softmax'))(x)        \n",
    "        # ctc inputs\n",
    "        labels = Input(name='the_labels', shape=[None,], dtype='int32')\n",
    "        input_length = Input(name='input_length', shape=[1], dtype='int32')\n",
    "        label_length = Input(name='label_length', shape=[1], dtype='int32')    \n",
    "#        label_length = np.array([self.max_label for _ in range(self.batch_size)])\n",
    "#         label_length=150\n",
    "        loss_out = Lambda(ctc_lambda_func, \n",
    "                          output_shape=(1,), \n",
    "                          name='ctc')([y_pred,\n",
    "                                        labels,\n",
    "                                        input_length,\n",
    "                                        label_length])        \n",
    "        self.tm = Model(inputs = i,\n",
    "                        outputs = y_pred)\n",
    "        self.m = Model(inputs = [i, \n",
    "                                 labels, \n",
    "                                 input_length, \n",
    "                                 label_length], \n",
    "                        outputs = loss_out)\n",
    "        return self.m, self.tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 113 examples\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'spec_gram' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8b047788f0de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-9142ce6c1bc9>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Generate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#for CTC you to give 4 arguments, I have only given inputs and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-9142ce6c1bc9>\u001b[0m in \u001b[0;36m__data_generation\u001b[0;34m(self, list_IDs_temp)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;31m#I have fixed the dimensions, you will need to change this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mspec_gram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m198\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec_gram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'spec_gram' referenced before assignment"
     ]
    }
   ],
   "source": [
    "training_generator = AudioDataGenerator( os.getcwd(), char_map)\n",
    "training_generator.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc(y_true, y_pred):\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.engine.training.Model at 0x7ff371b961d0>,\n",
       " <keras.engine.training.Model at 0x7ff371bc4be0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_ctc = model((101,198,1), 28)\n",
    "sr_ctc.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 101, 198, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d1 (Conv2D)                (None, 101, 198, 13) 338         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 101, 198, 13) 52          conv2d1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 101, 198, 13) 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d2 (Conv2D)                (None, 101, 198, 13) 4238        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 101, 198, 13) 52          conv2d2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 101, 198, 13) 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 606, 429)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d1 (Conv1D)                (None, 152, 200)     429200      reshape_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 152, 200)     800         conv1d1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 152, 200)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d2 (Conv1D)                (None, 38, 200)      200200      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 38, 200)      800         conv1d2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 38, 200)      0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 200, 38)      0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200, 23)      897         reshape_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_30 (Reshape)            (None, 23, 200)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 23, 256)      336896      reshape_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 23, 256)      0           bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 23, 256)      1024        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 23, 256)      394240      batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 23, 256)      0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 23, 256)      1024        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 23, 28)       7196        batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           time_distributed_10[0][0]        \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,376,957\n",
      "Trainable params: 1,375,081\n",
      "Non-trainable params: 1,876\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sr_ctc.m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_ctc.m.compile(loss = ctc, optimizer = 'adam', metrics = ['accuracy'])\n",
    "sr_ctc.tm.compile(loss = ctc, optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = sr_ctc.m.fit_generator(training_generator, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
